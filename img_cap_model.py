# -*- coding: utf-8 -*-
"""img_cap_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H5j5BDn05qvpIQC583xisIOiFSgZ9GDL
"""

import numpy as np
import os
import pandas as pd

from keras.preprocessing import image, sequence
from keras.layers import Dense, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate
from keras.models import Sequential, Model
from keras.optimizers import Adam
from keras.applications import ResNet50

import sys

file = open("myfile.txt", "r")
imgurl=file.read()

import base64
with open("image1.png","wb") as fh:
    fh.write(base64.b64decode(imgurl[22:]))

img="./image1.png"


pd_dataset = pd.read_csv("../ImageCaptioning/Flickr8k_text/flickr_8k_train_dataset.txt", delimiter='\t')
ds = pd_dataset.values

sentences = []
for ix in range(ds.shape[0]):
    sentences.append(ds[ix, 1])


words = [i.split() for i in sentences]

unique = []
for i in words:
    unique.extend(i)

unique = list(set(unique))

vocab_size = len(unique)

word_count_threshold = 10
word_counts = {}
nsents = 0
for sent in sentences:
    nsents += 1
    for w in sent.split(' '):
        word_counts[w] = word_counts.get(w, 0) + 1

vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]

#Vectorization
word_2_indices = {val:index for index, val in enumerate(vocab)}
indices_2_word = {index:val for index, val in enumerate(vocab)}

word_2_indices['UNK'] = 0
indices_2_word[0] = 'UNK'
word_2_indices['<start>']=1698
indices_2_word[1698] = '<start>'


vocab_size = len(word_2_indices.keys())


captions = np.load("../ImageCaptioning/CCaptions.npy")
next_words = np.load("../ImageCaptioning/NNextwords.npy")


images = np.load("../ImageCaptioning/images.npy")


imag = np.load("../ImageCaptioning/Image_names.npy")


embedding_size = 128
max_len = 40

image_model = Sequential()

image_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))
image_model.add(RepeatVector(max_len))



language_model = Sequential()

language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))
language_model.add(LSTM(256, return_sequences=True))
language_model.add(TimeDistributed(Dense(embedding_size)))




conca = Concatenate()([image_model.output, language_model.output])
x = LSTM(128, return_sequences=True)(conca)
x = LSTM(512, return_sequences=False)(x)
x = Dense(vocab_size)(x)
out = Activation('softmax')(x)
model = Model(inputs=[image_model.input, language_model.input], outputs = out)

model.load_weights("../ImageCaptioning/model_weightsF.h5")
model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])





def preprocessing(img_path):
    im = image.load_img(img_path, target_size=(224,224,3))
    im = image.img_to_array(im)
    im = np.expand_dims(im, axis=0)
    return im

def get_encoding(model, img):
    image = preprocessing(img)
    pred = model.predict(image).reshape(2048)
    return pred


resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')

test_img = get_encoding(resnet, img)

def predict_captions(image):
    start_word = ["<start>"]
    while True:
        par_caps = [word_2_indices[i] for i in start_word]
        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')
        preds = model.predict([np.array([image]), np.array(par_caps)])
        word_pred = indices_2_word[np.argmax(preds[0])]
        start_word.append(word_pred)

        if word_pred == "<end>" or len(start_word) > max_len:
            break

    return ' '.join(start_word[1:-1])

Argmax_Search = predict_captions(test_img)

from gtts import gTTS

language = 'en'

myobj = gTTS(text=Argmax_Search, lang=language, slow=False)
myobj.save("public/captionaudio.mp3")

print(Argmax_Search)
